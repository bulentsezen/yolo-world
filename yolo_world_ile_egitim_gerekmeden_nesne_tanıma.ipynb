{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Zero-Shot Object Detection with YOLO-World\n",
        "---\n",
        "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow/supervision/blob/develop/docs/notebooks/zero-shot-object-detection-with-yolo-world.ipynb)\n",
        "[![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/what-is-yolo-world/)\n",
        "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/AILab-CVC/YOLO-World)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2401.17270-b31b1b.svg)](https://arxiv.org/abs/2401.17270)\n",
        "\n",
        "Click the `Open in Colab` button to run the cookbook on Google Colab.\n",
        "\n",
        "<br>\n",
        "\n",
        "YOLO-World was designed to solve a limitation of existing zero-shot object detection models: speed. Whereas other state-of-the-art models use Transformers, a powerful but typically slower architecture, YOLO-World uses the faster CNN-based YOLO architecture.\n",
        "\n",
        "According to the paper YOLO-World reached between 35.4 AP with 52.0 FPS for the large version and 26.2 AP with 74.1 FPS for the small version. While the V100 is a powerful GPU, achieving such high FPS on any device is impressive.\n",
        "\n",
        "![visualization results of referring object detection](https://storage.googleapis.com/com-roboflow-marketing/supervision/cookbooks/yolo-world-visualization-results-of-referring-object-detection.png)"
      ],
      "metadata": {
        "id": "kR-PyK7YXPVZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Before you start\n",
        "\n",
        "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`."
      ],
      "metadata": {
        "id": "T-lkdAZUY73s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mP1Bv5jBXNbw"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** To make it easier for us to manage datasets, images and models we create a `HOME` constant."
      ],
      "metadata": {
        "id": "nySApmvdZDEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ],
      "metadata": {
        "id": "JGXqXrQfY2yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install required packages\n",
        "\n",
        "In this guide, we utilize two Python packages: `inference`, for executing zero-shot object detection using YOLO-World, and `supervision`, for post-processing and visualizing the detected objects."
      ],
      "metadata": {
        "id": "h1jNp-i3ZL0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q inference-gpu[yolo-world]==0.9.12rc1"
      ],
      "metadata": {
        "id": "zfc06V9QZFup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q supervision==0.19.0rc3"
      ],
      "metadata": {
        "id": "44LZf5qb3bet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "dnEI8EOpalbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import supervision as sv\n",
        "\n",
        "from tqdm import tqdm\n",
        "from inference.models.yolo_world.yolo_world import YOLOWorld"
      ],
      "metadata": {
        "id": "UIcZyEhuacFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download example data"
      ],
      "metadata": {
        "id": "ULzC4SZdbO-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SOURCE_IMAGE_PATH = f\"{HOME}/yemek.jpg\"\n",
        "SOURCE_VIDEO_PATH = f\"{HOME}/beyaz_ucak.mp4\""
      ],
      "metadata": {
        "id": "Kvq4jtTjg31-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** If you want to run the cookbook using your own file as input, simply upload video to Google Colab and replace `SOURCE_IMAGE_PATH` and `SOURCE_VIDEO_PATH` with the path to your file."
      ],
      "metadata": {
        "id": "M0_JmZ08gyYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Object Detection\n",
        "\n",
        "The Inference package provides the YOLO-World model in three versions: `S`, `M`, and `L`. You can load them by defining model_id as `yolo_world/s`, `yolo_world/m`, and `yolo_world/l`, respectively. The `ROBOFLOW_API_KEY` is not required to utilize this model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NLP6DK4YbVpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLOWorld(model_id=\"yolo_world/l\")"
      ],
      "metadata": {
        "id": "osrplWqebju0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOLO-World is a zero-shot model, enabling object detection without any training. You only need to define a prompt as a list of classes (things) you are searching for."
      ],
      "metadata": {
        "id": "vzZgYhZodzgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\"pasta\", \"soup\"]\n",
        "model.set_classes(classes)"
      ],
      "metadata": {
        "id": "UZTYuZlAdYhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We perform detection on our sample image. Then, we convert the result into a [`sv.Detections`](https://supervision.roboflow.com/latest/detection/core/) object, which will be useful in the later parts of the cookbook."
      ],
      "metadata": {
        "id": "fXLWvBQNgoV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread(SOURCE_IMAGE_PATH)\n",
        "results = model.infer(image)\n",
        "detections = sv.Detections.from_inference(results)"
      ],
      "metadata": {
        "id": "ZinEbFJsdp9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results we've obtained can be easily visualized with [`sv.BoundingBoxAnnotator`](https://supervision.roboflow.com/latest/annotators/#supervision.annotators.core.BoundingBoxAnnotator) and [`sv.LabelAnnotator`](https://supervision.roboflow.com/latest/annotators/#supervision.annotators.core.LabelAnnotator). We can adjust parameters such as line thickness, text scale, line and text color allowing for a highly tailored visualization experience."
      ],
      "metadata": {
        "id": "jiYw1EXzhpbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BOUNDING_BOX_ANNOTATOR = sv.BoundingBoxAnnotator(thickness=2)\n",
        "LABEL_ANNOTATOR = sv.LabelAnnotator(text_thickness=2, text_scale=1, text_color=sv.Color.BLACK)"
      ],
      "metadata": {
        "id": "37CMTxw0jSyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annotated_image = image.copy()\n",
        "annotated_image = BOUNDING_BOX_ANNOTATOR.annotate(annotated_image, detections)\n",
        "annotated_image = LABEL_ANNOTATOR.annotate(annotated_image, detections)\n",
        "sv.plot_image(annotated_image, (10, 10))"
      ],
      "metadata": {
        "id": "YkVoWxchher5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adjusting Confidence Level"
      ],
      "metadata": {
        "id": "KrferzhUnOk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that many classes from our prompt were not detected. This is because the default confidence threshold in Inference is set to `0.5`. Let's try significantly lowering this value. We've observed that the confidence returned by YOLO-World is significantly lower when querying for classes outside the [COCO](https://universe.roboflow.com/microsoft/coco) dataset."
      ],
      "metadata": {
        "id": "-aBRGrJnlvjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread(SOURCE_IMAGE_PATH)\n",
        "results = model.infer(image, confidence=0.08)\n",
        "detections = sv.Detections.from_inference(results)"
      ],
      "metadata": {
        "id": "XKAJJQVLiE6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, `sv.LabelAnnotator` displays only the names of objects. To also view the confidence levels associated with each detection, we must define custom `labels` and pass them to `sv.LabelAnnotator`."
      ],
      "metadata": {
        "id": "E2UGr46TmiaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [\n",
        "    f\"{classes[class_id]} {confidence:0.3f}\"\n",
        "    for class_id, confidence\n",
        "    in zip(detections.class_id, detections.confidence)\n",
        "]\n",
        "\n",
        "annotated_image = image.copy()\n",
        "annotated_image = BOUNDING_BOX_ANNOTATOR.annotate(annotated_image, detections)\n",
        "annotated_image = LABEL_ANNOTATOR.annotate(annotated_image, detections, labels=labels)\n",
        "sv.plot_image(annotated_image, (10, 10))"
      ],
      "metadata": {
        "id": "Zw9tE3fKl4R3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Non-Max Suppression (NMS) to Eliminate Double Detections\n",
        "\n",
        "To eliminate duplicates, we will use [Non-Max Suppression (NMS)](https://blog.roboflow.com/how-to-code-non-maximum-suppression-nms-in-plain-numpy). NMS evaluates the extent to which detections overlap using the Intersection over Union metric and, upon exceeding a defined threshold, treats them as duplicates. Duplicates are then discarded, starting with those of the lowest confidence. The value should be within the range `[0, 1]`. The smaller the value, the more restrictive the NMS.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t88HE-7FofwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread(SOURCE_IMAGE_PATH)\n",
        "results = model.infer(image, confidence=0.08)\n",
        "detections = sv.Detections.from_inference(results).with_nms(threshold=0.1)"
      ],
      "metadata": {
        "id": "pyyjlCkAl9g6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [\n",
        "    f\"{classes[class_id]} {confidence:0.3f}\"\n",
        "    for class_id, confidence\n",
        "    in zip(detections.class_id, detections.confidence)\n",
        "]\n",
        "\n",
        "annotated_image = image.copy()\n",
        "annotated_image = BOUNDING_BOX_ANNOTATOR.annotate(annotated_image, detections)\n",
        "annotated_image = LABEL_ANNOTATOR.annotate(annotated_image, detections, labels=labels)\n",
        "sv.plot_image(annotated_image, (10, 10))"
      ],
      "metadata": {
        "id": "no-BoxjTpJt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Video Processing"
      ],
      "metadata": {
        "id": "S-LLI26VvF7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The [`get_video_frames_generator`](https://supervision.roboflow.com/latest/utils/video/#supervision.utils.video.get_video_frames_generator) enables us to easily iterate over video frames. Let's create a video generator for our sample input file and display its first frame on the screen."
      ],
      "metadata": {
        "id": "E0rhepjGv9oe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "frame = next(generator)\n",
        "\n",
        "sv.plot_image(frame, (10, 10))"
      ],
      "metadata": {
        "id": "J2Tm9lcGpLF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ktpuUCPe_1vq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's update our list of classes. This time we are looking for `white airplane`. The rest of the code performing detection, filtering and visualization remains unchanged."
      ],
      "metadata": {
        "id": "Su3lwi6D62sR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\"white airplane\"]\n",
        "model.set_classes(classes)"
      ],
      "metadata": {
        "id": "bddF-z6WwAkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.infer(frame, confidence=0.002)\n",
        "detections = sv.Detections.from_inference(results).with_nms(threshold=0.1)"
      ],
      "metadata": {
        "id": "xQ0kV9huwmoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annotated_image = frame.copy()\n",
        "annotated_image = BOUNDING_BOX_ANNOTATOR.annotate(annotated_image, detections)\n",
        "annotated_image = LABEL_ANNOTATOR.annotate(annotated_image, detections)\n",
        "sv.plot_image(annotated_image, (10, 10))"
      ],
      "metadata": {
        "id": "AfWCMuq5wsuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Result"
      ],
      "metadata": {
        "id": "7SjPoFnLC9aI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we are ready to process our entire video. Now in truth we can appreciate the speed of YOLO-World."
      ],
      "metadata": {
        "id": "WxwNMXGECodq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TARGET_VIDEO_PATH = f\"{HOME}/beyaz_ucak_cikti_video.mp4\""
      ],
      "metadata": {
        "id": "p0aooVpaCSH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
        "\n",
        "width, height = video_info.resolution_wh\n",
        "frame_area = width * height\n",
        "frame_area\n",
        "\n",
        "with sv.VideoSink(target_path=TARGET_VIDEO_PATH, video_info=video_info) as sink:\n",
        "    for frame in tqdm(frame_generator, total=video_info.total_frames):\n",
        "        results = model.infer(frame, confidence=0.002)\n",
        "        detections = sv.Detections.from_inference(results).with_nms(threshold=0.1)\n",
        "        detections = detections[(detections.area / frame_area) < 0.10]\n",
        "\n",
        "        annotated_frame = frame.copy()\n",
        "        annotated_frame = BOUNDING_BOX_ANNOTATOR.annotate(annotated_frame, detections)\n",
        "        annotated_frame = LABEL_ANNOTATOR.annotate(annotated_frame, detections)\n",
        "        sink.write_frame(annotated_frame)"
      ],
      "metadata": {
        "id": "A8TzekpEwtTO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}